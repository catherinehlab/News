import feedparser
from openai import OpenAI
# import yagmail  # yagmail ì‚­ì œ
from datetime import datetime
from dotenv import load_dotenv
import os
import requests
from bs4 import BeautifulSoup
import time
import base64
from email.mime.text import MIMEText
from googleapiclient.discovery import build
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
import pickle
import re
import json
# pip install feedfinder2 í•„ìš”
try:
    from feedfinder2 import find_feeds
except ImportError:
    find_feeds = None

# === ì„¤ì • ===
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
RECIPIENT = os.getenv("RECIPIENT")
SENDER = os.getenv("SENDER")
# APP_PASSWORD = os.getenv("APP_PASSWORD")  # ì‚­ì œ

# === Gmail API ì¸ì¦ ===
SCOPES = ['https://www.googleapis.com/auth/gmail.send']

def gmail_authenticate():
    creds = None
    if os.path.exists('token.pickle'):
        with open('token.pickle', 'rb') as token:
            creds = pickle.load(token)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(
                'credentials.json', SCOPES)
            creds = flow.run_local_server(port=0)
        with open('token.pickle', 'wb') as token:
            pickle.dump(creds, token)
    return build('gmail', 'v1', credentials=creds)

# === ë‰´ìŠ¤ ì†ŒìŠ¤ êµ¬ì¡°í™” ===
NEWS_SOURCES = [
    # RSS ì§€ì›
    {
        "name": "ì‹í’ˆì™¸ì‹ê²½ì œ",
        "type": "rss",
        "url": "https://www.foodbank.co.kr/rss/all.xml",
        "keywords": ["ì™¸ì‹ì—…", "ìš”ì‹ì—…", "í–‰ì‚¬", "ì´ë²¤íŠ¸", "ë„¤íŠ¸ì›Œí‚¹", "êµë¥˜", "êµìœ¡", "ì´ë²¤íŠ¸", "í–‰ì‚¬"],
        "region": "êµ­ë‚´"
    },
    {
        "name": "í‘¸ë“œíˆ¬ë°ì´",
        "type": "rss",
        "url": "http://www.foodtoday.or.kr/rss/allArticle.xml",
        "keywords": ["ì™¸ì‹ì—…", "ìš”ì‹ì—…", "í–‰ì‚¬", "ì´ë²¤íŠ¸", "ë„¤íŠ¸ì›Œí‚¹", "êµë¥˜", "êµìœ¡", "ì´ë²¤íŠ¸", "í–‰ì‚¬"],
        "region": "êµ­ë‚´"
    },
    {
        "name": "ì‹í’ˆì €ë„",
        "type": "rss",
        "url": "http://www.foodnews.co.kr/rss/allArticle.xml",
        "keywords": ["ì™¸ì‹ì—…", "ìš”ì‹ì—…", "í–‰ì‚¬", "ì´ë²¤íŠ¸", "ë„¤íŠ¸ì›Œí‚¹", "êµë¥˜", "êµìœ¡", "ì´ë²¤íŠ¸", "í–‰ì‚¬"],
        "region": "êµ­ë‚´"
    },
    {
        "name": "ì‹í’ˆìŒë£Œì‹ ë¬¸",
        "type": "rss",
        "url": "http://www.thinkfood.co.kr/rss/allArticle.xml",
        "keywords": ["ì™¸ì‹ì—…", "ìš”ì‹ì—…", "í–‰ì‚¬", "ì´ë²¤íŠ¸", "ë„¤íŠ¸ì›Œí‚¹", "êµë¥˜", "êµìœ¡", "ì´ë²¤íŠ¸", "í–‰ì‚¬"],
        "region": "êµ­ë‚´"
    },
    # HTML í¬ë¡¤ë§ í•„ìš” (ì˜ˆì‹œ)
    {
        "name": "ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€ ë³´ë„ìë£Œ",
        "type": "html",
        "url": "https://www.mafra.go.kr/mafra/293/subview.do",
        "keywords": ["ì™¸ì‹ì—…", "ìš”ì‹ì—…", "í–‰ì‚¬", "ì´ë²¤íŠ¸", "ë„¤íŠ¸ì›Œí‚¹", "êµë¥˜", "êµìœ¡", "ì´ë²¤íŠ¸", "í–‰ì‚¬"],
        "region": "êµ­ë‚´"
    },
    {
        "name": "ì™¸ì‹ê²½ì˜",
        "type": "html",
        "url": "http://www.foodservice.co.kr/",
        "keywords": ["ì™¸ì‹ì—…", "ìš”ì‹ì—…", "í–‰ì‚¬", "ì´ë²¤íŠ¸", "ë„¤íŠ¸ì›Œí‚¹", "êµë¥˜", "êµìœ¡", "ì´ë²¤íŠ¸", "í–‰ì‚¬"],
        "region": "êµ­ë‚´"
    },
    {
        "name": "ë§¤ì¼ê²½ì œ ìœ í†µê²½ì œ",
        "type": "html",
        "url": "https://www.mk.co.kr/news/business/retail/",
        "keywords": ["ì™¸ì‹ì—…", "ìš”ì‹ì—…", "í–‰ì‚¬", "ì´ë²¤íŠ¸", "ë„¤íŠ¸ì›Œí‚¹", "êµë¥˜", "êµìœ¡", "ì´ë²¤íŠ¸", "í–‰ì‚¬"],
        "region": "êµ­ë‚´"
    },
    {
        "name": "í•œê²½ FOODì„¹ì…˜",
        "type": "html",
        "url": "https://www.hankyung.com/food",
        "keywords": ["ì™¸ì‹ì—…", "ìš”ì‹ì—…", "í–‰ì‚¬", "ì´ë²¤íŠ¸", "ë„¤íŠ¸ì›Œí‚¹", "êµë¥˜", "êµìœ¡", "ì´ë²¤íŠ¸", "í–‰ì‚¬"],
        "region": "êµ­ë‚´"
    },
    # í•´ì™¸ ì˜ˆì‹œ(ê¸°ì¡´ ìœ ì§€)
    {
        "name": "Nationâ€™s Restaurant News",
        "type": "rss",
        "url": "https://www.nrn.com/rss.xml",
        "keywords": ["restaurant", "event", "networking", "education", "news"],
        "region": "í•´ì™¸"
    },
    {
        "name": "Eater",
        "type": "rss",
        "url": "https://www.eater.com/rss/index.xml",
        "keywords": ["restaurant", "event", "networking", "education", "news"],
        "region": "í•´ì™¸"
    },
]

# === êµ¬ê¸€ Custom Search APIë¡œ ìƒˆ ì‚¬ì´íŠ¸ ìë™ ì¶”ê°€ ===
GOOGLE_API_KEY = "AIzaSyDU5fjMpbNl6fk9LuasX8s3F9woP_RmB9A"
GOOGLE_CX = "a0a8f206f396641ac"
SEARCH_KEYWORDS = [
    "ì™¸ì‹ì—… í–‰ì‚¬", "ì™¸ì‹ì—… ì´ë²¤íŠ¸", "ìš”ì‹ì—… í–‰ì‚¬", "ìš”ì‹ì—… ì´ë²¤íŠ¸", "ì™¸ì‹ì—… ë‰´ìŠ¤", "ìš”ì‹ì—…ë‰´ìŠ¤", "ìš”ì‹ì—… ì´ë²¤íŠ¸", "ì‹í’ˆê°œë°œ", "ì¼€ì´í„°ë§"
]

def get_domain(url):
    match = re.match(r"https?://([^/]+)/", url)
    if match:
        return match.group(1)
    return None

def update_news_sources_from_google():
    print("[LOG] êµ¬ê¸€ì—ì„œ ì™¸ì‹ì—… í–‰ì‚¬/ì´ë²¤íŠ¸ ê´€ë ¨ ì‚¬ì´íŠ¸ ìë™ íƒìƒ‰...")
    global NEWS_SOURCES
    existing_domains = set(get_domain(src["url"]) for src in NEWS_SOURCES)
    new_sources = []
    for keyword in SEARCH_KEYWORDS:
        params = {
            "key": GOOGLE_API_KEY,
            "cx": GOOGLE_CX,
            "q": keyword,
            "num": 10
        }
        resp = requests.get("https://www.googleapis.com/customsearch/v1", params=params)
        if resp.status_code != 200:
            print(f"[LOG] êµ¬ê¸€ ê²€ìƒ‰ ì‹¤íŒ¨: {resp.text}")
            continue
        data = resp.json()
        for item in data.get("items", []):
            link = item.get("link")
            domain = get_domain(link)
            if not domain or any(domain in get_domain(src["url"]) for src in NEWS_SOURCES):
                continue
            # RSS í”¼ë“œ ìë™ íƒì§€
            rss_url = None
            if find_feeds:
                try:
                    feeds = find_feeds(link)
                    if feeds:
                        rss_url = feeds[0]
                except Exception as e:
                    print(f"[LOG] RSS íƒì§€ ì‹¤íŒ¨: {e}")
            if rss_url:
                new_sources.append({
                    "name": domain,
                    "type": "rss",
                    "url": rss_url,
                    "keywords": ["ì™¸ì‹ì—…", "ìš”ì‹ì—…", "í–‰ì‚¬", "ì´ë²¤íŠ¸", "ë„¤íŠ¸ì›Œí‚¹", "êµë¥˜", "êµìœ¡", "ì´ë²¤íŠ¸", "í–‰ì‚¬"],
                    "region": "êµ­ë‚´"
                })
            else:
                new_sources.append({
                    "name": domain,
                    "type": "html",
                    "url": link,
                    "keywords": ["ì™¸ì‹ì—…", "ìš”ì‹ì—…", "í–‰ì‚¬", "ì´ë²¤íŠ¸", "ë„¤íŠ¸ì›Œí‚¹", "êµë¥˜", "êµìœ¡", "ì´ë²¤íŠ¸", "í–‰ì‚¬"],
                    "region": "êµ­ë‚´"
                })
    # ì¤‘ë³µ ë„ë©”ì¸ ì œê±°
    added_domains = set(get_domain(src["url"]) for src in NEWS_SOURCES)
    for src in new_sources:
        d = get_domain(src["url"])
        if d and d not in added_domains:
            NEWS_SOURCES.append(src)
            added_domains.add(d)
    print(f"[LOG] êµ¬ê¸€ ê²€ìƒ‰ ê¸°ë°˜ êµ­ë‚´ ì†ŒìŠ¤ {len(new_sources)}ê°œ ìë™ ì¶”ê°€ ì™„ë£Œ.")

# === 1. ë‰´ìŠ¤ í¬ë¡¤ë§ (RSS/HTML ìë™ ë¶„ê¸°, ì»¤ìŠ¤í…€ íŒŒì„œ êµ¬ì¡°í™”) ===
def fetch_news():
    print("[LOG] ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
    all_news = []
    for source in NEWS_SOURCES:
        print(f"[LOG] {source['name']} ({source['type']}) í¬ë¡¤ë§ ì¤‘...")
        try:
            if source["type"] == "rss":
                feed = feedparser.parse(source["url"])
                for entry in feed.entries:
                    title = entry.title
                    summary = entry.summary if hasattr(entry, 'summary') else ''
                    link = entry.link
                    # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ (ê°€ëŠ¥í•œ ê²½ìš°)
                    pub_date = ''
                    if hasattr(entry, 'published'):
                        pub_date = entry.published
                    elif hasattr(entry, 'updated'):
                        pub_date = entry.updated
                    # í‚¤ì›Œë“œ í•„í„°
                    if any(k in title or k in summary for k in source["keywords"]):
                        all_news.append({
                            "title": title,
                            "link": link,
                            "summary": summary,
                            "source": source["name"],
                            "date": pub_date,
                            "region": source["region"]
                        })
            elif source["type"] == "html":
                # ì»¤ìŠ¤í…€ íŒŒì„œ êµ¬ì¡°: ì‚¬ì´íŠ¸ë³„ë¡œ í•¨ìˆ˜ ë¶„ë¦¬ ê°€ëŠ¥
                if source["name"] == "ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€ ë³´ë„ìë£Œ":
                    all_news.extend(parse_mafra_news(source))
                else:
                    # ê¸°ë³¸ HTML íŒŒì„œ (í•„ìš”ì‹œ í™•ì¥)
                    resp = requests.get(source["url"], timeout=10)
                    soup = BeautifulSoup(resp.text, "html.parser")
                    articles = soup.select("a")
                    for a in articles[:10]:
                        title = a.get_text(strip=True)
                        link = a.get("href")
                        if not link or not title:
                            continue
                        if not link.startswith("http"):
                            link = source["url"] + link
                        if any(k in title for k in source["keywords"]):
                            all_news.append({
                                "title": title,
                                "link": link,
                                "summary": title,
                                "source": source["name"],
                                "date": '',
                                "region": source["region"]
                            })
            print(f"[LOG] {source['name']} í¬ë¡¤ë§ ì™„ë£Œ.")
            time.sleep(1)  # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€
        except Exception as e:
            print(f"[LOG] {source['name']} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    print(f"[LOG] ì „ì²´ ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ. {len(all_news)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘.")
    return all_news

# === ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€ ë³´ë„ìë£Œ ì»¤ìŠ¤í…€ íŒŒì„œ ===
def parse_mafra_news(source):
    news = []
    try:
        resp = requests.get(source["url"], timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        articles = soup.select(".bd-list .bd-title a")
        for a in articles[:10]:
            title = a.get_text(strip=True)
            link = a.get("href")
            if not link or not title:
                continue
            if not link.startswith("http"):
                link = "https://www.mafra.go.kr" + link
            # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ (ê°™ì€ í–‰ì˜ ë‚ ì§œ span ë“±ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥)
            date = ''
            row = a.find_parent("tr")
            if row:
                date_td = row.find_all("td")
                if len(date_td) > 2:
                    date = date_td[2].get_text(strip=True)
            if any(k in title for k in source["keywords"]):
                news.append({
                    "title": title,
                    "link": link,
                    "summary": title,
                    "source": source["name"],
                    "date": date,
                    "region": source["region"]
                })
    except Exception as e:
        print(f"[LOG] ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€ íŒŒì‹± ì‹¤íŒ¨: {e}")
    return news

# === 2. ì¤‘ë³µ ì œê±° ===
def deduplicate_news(news_items):
    seen = set()
    unique_news = []
    for item in news_items:
        key = (item["title"], item["link"])
        if key not in seen:
            unique_news.append(item)
            seen.add(key)
    print(f"[LOG] ì¤‘ë³µ ì œê±° í›„ {len(unique_news)}ê°œ ê¸°ì‚¬ ë‚¨ìŒ.")
    return unique_news

# === 3. GPTë¡œ ìš”ì•½ ë° í•´ì™¸ ë²ˆì—­ ===
def summarize_news(news_items):
    print("[LOG] ë‰´ìŠ¤ ìš”ì•½ ì‹œì‘...")
    summaries = []
    for idx, item in enumerate(news_items[:10]):
        content = item["summary"]
        prompt = f"[{item['source']}] {item['title']}\n\në‹¤ìŒ ë‚´ìš©ì„ 3ì¤„ë¡œ ìš”ì•½í•´ì¤˜:\n\n{content}"
        print(f"[LOG] {idx+1}ë²ˆì§¸ ë‰´ìŠ¤ ìš”ì•½ ìš”ì²­ ì¤‘...")
        try:
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}]
            )
            summary = response.choices[0].message.content.strip()
            # í•´ì™¸ ë‰´ìŠ¤ëŠ” í•œêµ­ì–´ ë²ˆì—­ ì¶”ê°€
            if item.get("region") == "í•´ì™¸":
                translate_prompt = f"ë‹¤ìŒ ì˜ì–´ ìš”ì•½ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì¤˜.\n\n{summary}"
                print(f"[LOG] {idx+1}ë²ˆì§¸ í•´ì™¸ ë‰´ìŠ¤ ë²ˆì—­ ìš”ì²­ ì¤‘...")
                tr_response = client.chat.completions.create(
                    model="gpt-4",
                    messages=[{"role": "user", "content": translate_prompt}]
                )
                summary = tr_response.choices[0].message.content.strip()
        except Exception as e:
            summary = f"ìš”ì•½ ì‹¤íŒ¨: {e}"
        summaries.append({
            "title": item["title"],
            "summary": summary,
            "link": item["link"],
            "source": item["source"],
            "date": item["date"],
            "region": item.get("region", "êµ­ë‚´")
        })
        print(f"[LOG] {idx+1}ë²ˆì§¸ ë‰´ìŠ¤ ìš”ì•½ ì™„ë£Œ.")
    print(f"[LOG] ë‰´ìŠ¤ ìš”ì•½ ì „ì²´ ì™„ë£Œ. {len(summaries)}ê°œ ìš”ì•½.")
    return summaries

# === 4. ì´ë©”ì¼ ìƒì„± (êµ­ë‚´/í•´ì™¸ ì„¹ì…˜ êµ¬ë¶„, ì¹´ë“œí˜•, ë‚ ì§œ í¬í•¨) ===
def build_email(summaries):
    print("[LOG] ì´ë©”ì¼ ìƒì„± ì‹œì‘...")
    today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
    # êµ­ë‚´/í•´ì™¸ë¡œ ê·¸ë£¹í•‘
    grouped = {"êµ­ë‚´": [], "í•´ì™¸": []}
    for item in summaries:
        grouped[item["region"]].append(item)
    # ì¹´ë“œí˜• ìŠ¤íƒ€ì¼ CSS
    style = """
    <style>
    .news-category {margin-bottom: 32px;}
    .news-title {font-size: 1.2em; font-weight: bold; margin-bottom: 8px;}
    .news-card {
        border: 1px solid #e0e0e0;
        border-radius: 10px;
        padding: 16px;
        margin-bottom: 16px;
        background: #fafbfc;
        box-shadow: 0 2px 8px rgba(0,0,0,0.04);
    }
    .news-date {color: #888; font-size: 0.95em; margin-bottom: 6px;}
    .news-link {margin-top: 8px; display: inline-block; color: #1976d2; text-decoration: none;}
    </style>
    """
    html = f"<h2>ğŸ“¬ {today} ìš”ì‹ì—… ë‰´ìŠ¤ ìš”ì•½</h2>" + style
    # êµ­ë‚´ ì„¹ì…˜
    if grouped["êµ­ë‚´"]:
        html += '<div class="news-category"><div class="news-title">[ êµ­ë‚´ ]</div>'
        for item in grouped["êµ­ë‚´"]:
            html += '<div class="news-card">'
            if item["date"]:
                html += f'<div class="news-date">{item["date"]}</div>'
            html += f'<div><b>{item["title"]}</b></div>'
            html += f'<div>{item["summary"]}</div>'
            html += f'<a class="news-link" href="{item["link"]}">ì›ë¬¸ ë³´ê¸°</a>'
            html += '</div>'
        html += '</div>'
    # í•´ì™¸ ì„¹ì…˜
    if grouped["í•´ì™¸"]:
        html += '<div class="news-category"><div class="news-title">[ í•´ì™¸ ]</div>'
        for item in grouped["í•´ì™¸"]:
            html += '<div class="news-card">'
            if item["date"]:
                html += f'<div class="news-date">{item["date"]}</div>'
            html += f'<div><b>{item["title"]}</b></div>'
            html += f'<div>{item["summary"]}</div>'
            html += f'<a class="news-link" href="{item["link"]}">ì›ë¬¸ ë³´ê¸°</a>'
            html += '</div>'
        html += '</div>'
    html += "<p>ë§¤ì¼ ì˜¤ì „ 9ì‹œì— ìë™ ë°œì†¡ë©ë‹ˆë‹¤.</p>"
    print("[LOG] ì´ë©”ì¼ ìƒì„± ì™„ë£Œ.")
    return html

# === 5. ë©”ì¼ ì „ì†¡ (Gmail API) ===
def send_email(html):
    print("[LOG] ì´ë©”ì¼ ì „ì†¡ ì‹œì‘...")
    service = gmail_authenticate()
    message = MIMEText(html, 'html')
    message['to'] = RECIPIENT
    message['from'] = SENDER
    message['subject'] = "ğŸ“¬ ìš”ì‹ì—… ë‰´ìŠ¤ ìš”ì•½"
    raw = base64.urlsafe_b64encode(message.as_bytes()).decode()
    body = {'raw': raw}
    sent = service.users().messages().send(userId='me', body=body).execute()
    print(f"[LOG] ì´ë©”ì¼ ì „ì†¡ ì™„ë£Œ. Message Id: {sent['id']}")

# === ì‹¤í–‰ ===
if __name__ == "__main__":
    print("[LOG] í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì‹œì‘.")
    print("SENDER:", SENDER)
    # print("APP_PASSWORD:", APP_PASSWORD[:4] + "************") # ì‚­ì œ
    print("RECIPIENT:", RECIPIENT)
    update_news_sources_from_google()
    news = fetch_news()
    if news:
        news = deduplicate_news(news)
        summaries = summarize_news(news)
        html = build_email(summaries)
        send_email(html)
    else:
        print("[LOG] ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    print("[LOG] í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¢…ë£Œ.")
